## What is Airflow?

Apache Airflow is a platform to **author, schedule, and monitor** data workflows as **DAGs** (Directed Acyclic Graphs). Each node is a **Task**. Airflow handles dependencies, retries, scheduling, logging, and UI visualizations for you.

---

## Architecture (Big Picture)

> Think: **Scheduler** decides *what/when*, **Executor/Workers** do the *work*, **Webserver** shows the *UI*, **Metadata DB** remembers *state*, and a **DAG folder** provides *your code*.

**Basic deployment** (single box):

![Basic Airflow architecture](https://airflow.apache.org/docs/apache-airflow/stable/_images/diagram_basic_airflow_architecture.png)

**Distributed deployment** (separate roles/components):

![Distributed Airflow architecture](https://airflow.apache.org/docs/apache-airflow/stable/_images/diagram_distributed_airflow_architecture.png)

**UI — DAGs list**:

![Airflow UI DAG list](https://airflow.apache.org/docs/apache-airflow/stable/_images/dags.png)

References: Airflow docs → *Architecture Overview*, *Core Concepts*.

---

## Core Concepts (TL;DR)

* **DAG**: a Python-defined graph of tasks, no cycles.
* **Task**: a unit of work. Built from:

  * **Operators** (e.g., `PythonOperator`, `BashOperator`, `SqlOperator`, provider operators),
  * **Sensors** (wait for an external event),
  * **TaskFlow `@task`** (Python functions as tasks, passes data via XCom).
* **Dependencies**: set with `t1 >> t2` (or `set_downstream`/`set_upstream`).
* **Scheduling**: `schedule_interval`/`schedule` (cron, presets, or `None` for manual), **data interval** matters.
* **XCom**: lightweight data passing (metadata/small payloads) between tasks.
* **Connections/Variables**: central config (credentials, params) managed in UI or env/secret backends.
* **Pools**: concurrency limits.
* **Trigger Rules / Branching**: control flow (e.g., run if any upstream failed, branch on conditions).
* **Datasets** (2.4+): DAGs can *produce/consume* datasets for event-driven orchestration.

---

## Install & Quickstart

> Use the official constraints to avoid dependency conflicts.

**Local (pip):**

```bash
python -m venv .venv && source .venv/bin/activate
pip install "apache-airflow==2.9.3" \
  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.10.txt"

export AIRFLOW_HOME=$(pwd)/airflow_home
airflow db init

# Create an admin user
airflow users create \
  --username admin --password admin \
  --firstname Admin --lastname User \
  --role Admin --email admin@example.com

# Run webserver & scheduler (in separate terminals)
airflow webserver --port 8080
airflow scheduler
```

Open: [http://localhost:8080](http://localhost:8080)

**Docker Compose (alternative):**

* Clone the official `docker-compose.yaml` from Airflow’s quick-start guide.
* `docker compose up airflow-init` then `docker compose up`.

---

## Project Layout (example)

```
├─ dags/
│  └─ etl_orders.py           # our example DAG
├─ data/
│  ├─ raw/                    # generated by the DAG
│  └─ output/                 # generated by the DAG
└─ airflow_home/              # Airflow runtime dir (db, logs, etc.)
```

> Put your DAG files in the folder Airflow reads (default `AIRFLOW_HOME/dags` or configured `dags_folder`).

---

## End‑to‑End Example: Mini ETL (generate → transform → load → report)

This example DAG **creates sample orders data**, **transforms** it with pandas, **loads** a daily report, and **prints** a summary. It also demonstrates **dependencies**, **TaskFlow API**, **XCom**, **retry** settings, **TaskGroup**, and a `FileSensor` (optional) to wait for a file.

### Requirements (operators & libs)

```bash
pip install pandas
# (Airflow installed already)
```

### The DAG (drop into `dags/etl_orders.py`)

```python
from __future__ import annotations
from datetime import datetime, timedelta
from pathlib import Path
import json
import pandas as pd

from airflow import DAG
from airflow.decorators import task, dag, task_group
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.models import Variable

# Paths (inside your local project)
BASE = Path("/opt/airflow" if Path("/opt/airflow").exists() else Path.cwd())
DATA = BASE / "data"
RAW = DATA / "raw"
OUT = DATA / "output"
RAW.mkdir(parents=True, exist_ok=True)
OUT.mkdir(parents=True, exist_ok=True)

DEFAULT_ARGS = {
    "owner": "data-eng",
    "retries": 2,
    "retry_delay": timedelta(seconds=10),
}

@dag(
    dag_id="etl_orders",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",  # run daily; set to None for manual
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["example", "etl", "pandas"],
)
def etl_orders():
    """Generate → transform → load → report.
    Demonstrates dependencies, TaskFlow, TaskGroup, XCom, sensor, BashOperator.
    """

    # (Optional) Wait for a config file to appear before generating data
    wait_for_cfg = FileSensor(
        task_id="wait_for_config",
        filepath=str(DATA / "config.json"),
        poke_interval=5,
        timeout=60,
        mode="reschedule",  # frees worker while waiting
        soft_fail=True,      # don’t fail the whole DAG if it’s missing
    )

    @task
    def make_config():
        cfg = {"num_rows": 30, "seed": 123}
        (DATA / "config.json").write_text(json.dumps(cfg, indent=2))
        return cfg

    @task
    def generate_orders(cfg: dict) -> str:
        """Create a small CSV with random-ish order data."""
        import random
        random.seed(cfg["seed"])
        rows = []
        for i in range(cfg["num_rows"]):
            rows.append({
                "order_id": i + 1,
                "customer": random.choice(["Alice", "Bob", "Cara", "Dio"]),
                "country": random.choice(["US", "IN", "GB", "DE"]),
                "amount": round(random.uniform(10, 200), 2),
                "ts": datetime.utcnow().isoformat(),
            })
        df = pd.DataFrame(rows)
        out = RAW / "orders.csv"
        df.to_csv(out, index=False)
        return str(out)

    @task_group(group_id="transform")
    def transform_group(csv_path: str):
        @task
        def load_csv(path: str) -> str:
            # validate file exists
            assert Path(path).exists(), f"Missing file: {path}"
            return path

        @task
        def summarize(path: str) -> str:
            df = pd.read_csv(path)
            # Simple analytics: total by country and overall stats
            by_country = df.groupby("country")["amount"].agg(["count", "sum", "mean"]).reset_index()
            by_country.rename(columns={"count": "orders", "sum": "revenue", "mean": "avg"}, inplace=True)

            overall = {
                "orders": int(df.shape[0]),
                "revenue": round(float(df["amount"].sum()), 2),
                "avg": round(float(df["amount"].mean()), 2),
            }

            out_csv = OUT / "daily_country_revenue.csv"
            by_country.to_csv(out_csv, index=False)

            # return both artifacts via XCom (path + inline json)
            return json.dumps({"report": str(out_csv), "overall": overall})

        loaded = load_csv(csv_path)
        summary = summarize(loaded)
        return summary

    @task
    def publish_report(summary_json: str) -> str:
        info = json.loads(summary_json)
        report_path = info["report"]
        overall = info["overall"]
        msg = (
            f"Report saved: {report_path}\n"
            f"Overall Orders: {overall['orders']}\n"
            f"Total Revenue: {overall['revenue']}\n"
            f"Average Ticket: {overall['avg']}\n"
        )
        print(msg)
        # also write a small markdown summary for README preview
        md = OUT / "summary.md"
        md.write_text("# Daily Orders Summary\n\n" + msg)
        return str(md)

    # Example Bash task (e.g., list outputs)
    list_outputs = BashOperator(
        task_id="list_outputs",
        bash_command=f"ls -lah {OUT}",
    )

    # Dependency graph (mix of >> and TaskFlow wiring)
    cfg = make_config()
    gen = generate_orders(cfg)

    # Sensor and generator both must complete before transform
    ready = [wait_for_cfg, gen]

    summary_json = transform_group(gen)  # TaskGroup returns XCom str
    summary_md = publish_report(summary_json)

    # Set dependencies explicitly
    ready >> summary_json  # wait sensor + generator before transform
    summary_md >> list_outputs

etl_orders = etl_orders()
```

### What this DAG does

1. **`make_config`** writes `data/config.json` (seed, row count).
2. **`generate_orders`** writes `data/raw/orders.csv` with random sample orders.
3. **`transform_group`**

   * `load_csv` validates input file,
   * `summarize` writes `data/output/daily_country_revenue.csv` and returns overall stats via XCom.
4. **`publish_report`** prints a short summary and writes `data/output/summary.md`.
5. **`list_outputs`** runs a simple `ls` to show produced artifacts.

### Expected output (example)

`data/output/daily_country_revenue.csv` (first rows):

```csv
country,orders,revenue,avg
DE,8,762.55,95.32
GB,5,410.77,82.15
IN,9,801.06,89.01
US,8,728.44,91.06
```

Console log excerpt from **`publish_report`**:

```
Report saved: /.../data/output/daily_country_revenue.csv
Overall Orders: 30
Total Revenue: 2702.82
Average Ticket: 90.09
```

> Tip: Open the DAG in the UI → **Graph** view to see dependencies; click tasks for logs/XCom.

---

## Setting Dependencies (cheatsheet)

```python
# linear
extract >> transform >> load

# fan-out and fan-in
start >> [branch_a, branch_b]
[branch_a, branch_b] >> join

# methods (equivalent)
start.set_downstream([branch_a, branch_b])
join.set_upstream([branch_a, branch_b])
```

---

## Passing Data (XCom & TaskFlow)

* With TaskFlow `@task`, return values become XComs automatically (JSON‑serializable recommended).
* For larger data, write to storage (S3/GCS/Azure/DB) and pass a *pointer/path* via XCom.

```python
@task
def step1() -> dict:
    return {"path": "/data/file.parquet"}

@task
def step2(meta: dict):
    df = pd.read_parquet(meta["path"])  # pull from storage
```

---

## Scheduling & Backfills

* Use `schedule="@daily"` (or cron like `"0 6 * * *"`).
* `start_date` + `schedule` define **data intervals**.
* `catchup=True` will backfill all past intervals since `start_date`.

**Manual backfill:**

```bash
airflow dags backfill -s 2024-01-01 -e 2024-01-07 etl_orders
```

---

## Reliability & Control Flow

* **Retries**: `retries`, `retry_delay` in `default_args`.
* **SLA**: `sla=timedelta(minutes=10)` to flag late tasks.
* **Trigger Rules**: e.g., `all_done`, `one_failed`.
* **Branching**: `BranchPythonOperator` to pick a path.

```python
from airflow.operators.branch import BranchPythonOperator

choose = BranchPythonOperator(
    task_id="choose_path",
    python_callable=lambda: "do_a" if cond() else "do_b",
)
```

---

## Connections, Variables, Secrets

* Manage in **Admin → Connections/Variables**.
* Reference in code:

```python
from airflow.hooks.base import BaseHook
conn = BaseHook.get_connection("my_postgres")

from airflow.models import Variable
api_key = Variable.get("SOME_API_KEY", default_var="dev")
```

* For production, wire a **Secrets Backend** (AWS/GCP/Azure Vaults or env vars) so credentials aren’t stored in plain text.

---

## Executors (how tasks run)

* **Sequential/LocalExecutor**: simple/local dev.
* **CeleryExecutor**: distributed workers + message queue.
* **KubernetesExecutor**: tasks run as pods; good isolation/auto-scaling.

> Choose based on scale & infra. Ensure workers can **read DAGs** and reach the **metadata DB**.

---

## Monitoring, Logs & Metrics

* Logs in `AIRFLOW_HOME/logs` (or remote logs: S3/GCS/Azure).
* Metrics via StatsD/Prometheus + Grafana.
* UI: Graph, Gantt, Tree, Task duration, Code, Logs.

---

## Testing & CI

* **Static import test** (fail fast):

```bash
python -c "from airflow.models import DagBag; import sys; sys.exit(0 if DagBag('/path/to/dags').import_errors == {} else 1)"
```

* **Unit test a task** (isolate logic in pure functions and test them directly). For DAG structure, assert task ids & dependencies with `DagBag` in pytest.
* Add this to CI (GitHub Actions) to validate DAGs on PRs.

---

## Common CLI

```bash
# List dags & tasks
airflow dags list
airflow tasks list etl_orders --tree

# Trigger now
airflow dags trigger etl_orders

# Show logs
airflow tasks logs etl_orders publish_report 2024-07-01
```

---

## Troubleshooting Quickies

* **Nothing runs** → is the **scheduler** running? Any import errors in `DAGs` view? Check `logs/scheduler`.
* **Task stuck in queued** → executor misconfig, or workers cannot reach DB/DAGs.
* **Timezone confusion** → Airflow uses UTC internally; display can be localized.
* **Random import failures** → pin Airflow + constraints; restart scheduler after adding deps.

---

## License

These notes embed diagrams/screenshots from the official Apache Airflow documentation (© The Apache Software Foundation, used under the site’s documentation license).
